# ============================================
# Sekha LLM Bridge Environment Configuration
# ============================================

# Quick Start (Ollama Only):
# The bridge auto-configures for Ollama if no config.yaml is present.
# Simply set OLLAMA_BASE_URL and model names below.

# Advanced Setup:
# For multi-provider configurations (OpenAI, Anthropic, etc.),
# copy config.yaml.example to config.yaml and customize.

# ============================================
# Provider Configuration
# ============================================

# Ollama Configuration
OLLAMA_BASE_URL=http://ollama:11434

# Model Names
# These are used when no config.yaml is present
EMBEDDING_MODEL=nomic-embed-text
CHAT_MODEL=llama3.1:8b
VISION_MODEL=  # Optional: llama3.2-vision:11b

# Cloud Provider API Keys (Optional)
# Uncomment and set if using cloud providers via config.yaml
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# GROQ_API_KEY=gsk_...
# OPENROUTER_API_KEY=sk-or-...

# ============================================
# Server Configuration
# ============================================

HOST=0.0.0.0
PORT=5001
WORKERS=4
LOG_LEVEL=INFO

# ============================================
# Background Tasks (Redis/Celery)
# ============================================

REDIS_URL=redis://localhost:6379/0
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/1

# ============================================
# Advanced Configuration
# ============================================

# Config File Path (optional)
# If set and file exists, YAML config takes precedence over env vars
# CONFIG_PATH=config.yaml
